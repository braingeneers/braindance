{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "Detect and save sequences in pre-recording\n",
    "\n",
    "This is the same code as detect_sequences.ipynb, but the cells are separated, so it is easier to test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Globals setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: MKL_NUM_THREADS=1\n",
      "env: NUMEXPR_NUM_THREADS=1\n",
      "env: OMP_NUM_THREADS=1\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "%env MKL_NUM_THREADS=1\n",
    "%env NUMEXPR_NUM_THREADS=1\n",
    "%env OMP_NUM_THREADS=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mea/anaconda3/envs/brain_dance/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/mea/anaconda3/envs/brain_dance/lib/python3.11/site-packages/torch_tensorrt/fx/tracer/acc_tracer/acc_ops.py:895: UserWarning: Unable to import torchvision related libraries.: operator torchvision::nms does not exist. Please install torchvision lib in order to lower stochastic_depth\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "from importlib import reload\n",
    "from multiprocessing import Pool\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from spikeinterface.extractors import MaxwellRecordingExtractor\n",
    "from tqdm import tqdm\n",
    "\n",
    "%autoreload 2\n",
    "from braindance.core.spikesorter.manuscript_code import utils\n",
    "from braindance.core.spikesorter.manuscript_code import si_rec13 as F  # This forces you to manually reload every time modification happens (prevents forgetfulness errors)\n",
    "# from src.sorters.base import Unit\n",
    "\n",
    "from braindance.core.spikedetector.model2 import ModelSpikeSorter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load recording\n",
    "RECORDING_PATH = \"data.raw.h5\"\n",
    "##    \n",
    "RECORDING = MaxwellRecordingExtractor(RECORDING_PATH)\n",
    "SAMP_FREQ = round(RECORDING.get_sampling_frequency() / 1000)  # kHz\n",
    "NUM_ELECS = RECORDING.get_num_channels()\n",
    "ELEC_LOCS = RECORDING.get_channel_locations()\n",
    "\n",
    "assert SAMP_FREQ <= 35, \"SAMP_FREQ must be in kHz\"\n",
    "if SAMP_FREQ not in {20, 30}:\n",
    "    print(\"NEED TO CHANGE FRONT_BUFFER AND OUTPUT_WINDOW_HALF_SIZE TO MODEL'S VALUES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-recording is greater than five minutes. Using last five minutes to detect sequences\n"
     ]
    }
   ],
   "source": [
    "if RECORDING.get_total_duration() >= 5 * 60:  # Recording is greater than five minutes\n",
    "    training_duration_ms = RECORDING.get_total_samples(\n",
    "    ) / RECORDING.get_sampling_frequency() * 1000\n",
    "    TRAINING_MS = (training_duration_ms - 5*60*1000, training_duration_ms)  # Last 5 minute of first patch\n",
    "    TRACES_TRAINING_MS = (50, 5*60*1000)  # Rel to scaled_traces\n",
    "    print(\"Pre-recording is greater than five minutes. Using last five minutes to detect sequences\")\n",
    "else:\n",
    "    TRAINING_MS = (0, RECORDING.get_total_duration() * 1000)\n",
    "    TRACES_TRAINING_MS = (50, RECORDING.get_total_duration() * 1000)\n",
    "    \n",
    "TESTING_MS = (-1, -1)  # Not used for patch recordings\n",
    "# TESTING_MS = (training_duration_ms, RECORDING.get_total_duration() * 1000)  # 5 min to 10 min in recording (in ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/MEAprojects/BrainDance/braindance/core/data/test_rt_sort2\n",
      "/data/MEAprojects/BrainDance/braindance/core/data/test_rt_sort2/dl_model\n"
     ]
    }
   ],
   "source": [
    "ROOT_PATH = Path(\"/data/MEAprojects/BrainDance/braindance/core/data/test_rt_sort2\")\n",
    "ROOT_PATH_MODEL = ROOT_PATH / \"dl_model\"\n",
    "MODEL_PATH = Path(\"/data/MEAprojects/BrainDance/braindance/core/spikedetector/model_0_4_4_5118\")\n",
    "\n",
    "STRINGENT_THRESH = 0.275\n",
    "STRINGENT_THRESH_LOGIT = F.sigmoid_inverse(STRINGENT_THRESH)\n",
    "LOOSE_THRESH = 0.1 \n",
    "LOOSE_THRESH_LOGIT = F.sigmoid_inverse(LOOSE_THRESH)\n",
    "\n",
    "INFERENCE_SCALING_NUMERATOR = 12.6 \n",
    "\n",
    "FRONT_BUFFER = round(2*SAMP_FREQ)\n",
    "OUTPUT_WINDOW_HALF_SIZE = round(3*SAMP_FREQ)\n",
    "PRE_MEDIAN_FRAMES = round(50 * SAMP_FREQ)\n",
    "\n",
    "## No user inputs below\n",
    "ROOT_PATH.mkdir(exist_ok=True, parents=True)\n",
    "ROOT_PATH_MODEL.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "SCALED_TRACES_PATH = ROOT_PATH_MODEL / \"scaled_traces.npy\"\n",
    "\n",
    "MODEL_TRACES_PATH = ROOT_PATH_MODEL / \"model_traces.npy\"\n",
    "MODEL_OUTPUTS_PATH = ROOT_PATH_MODEL / \"model_outputs.npy\" \n",
    "\n",
    "ALL_CROSSINGS_PATH  = ROOT_PATH_MODEL / \"all_crossings.npy\"\n",
    "ELEC_CROSSINGS_IND_PATH = ROOT_PATH_MODEL / \"elec_crossings_ind.npy\"\n",
    "\n",
    "print(ROOT_PATH)\n",
    "print(ROOT_PATH_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.RECORDING = RECORDING\n",
    "F.NUM_ELECS = NUM_ELECS\n",
    "F.ELEC_LOCS = ELEC_LOCS\n",
    "F.SAMP_FREQ = SAMP_FREQ\n",
    "F.FRONT_BUFFER = FRONT_BUFFER\n",
    "F.INFERENCE_SCALING_NUMERATOR = INFERENCE_SCALING_NUMERATOR\n",
    "F.PRE_MEDIAN_FRAMES = PRE_MEDIAN_FRAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For RT-Sort manuscript: measure time to detect sequences\n",
    "import time\n",
    "\n",
    "class Stopwatch:\n",
    "    def __init__(self):\n",
    "        self.duration = 0\n",
    "        self.start_time = 0\n",
    "    def start(self):\n",
    "        self.start_time = time.time()\n",
    "    def stop(self):\n",
    "        stop_time = time.time()\n",
    "        self.duration += stop_time - self.start_time\n",
    "\n",
    "stopwatch = Stopwatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alllocating memory for traces ...\n",
      "Extracting traces ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:08<00:00,  7.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DL model ...\n",
      "Allocating memory to save model traces and outputs ...\n",
      "Inference scaling: 1.8\n",
      "Running model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49999/49999 [01:46<00:00, 467.48it/s]\n",
      "100%|██████████| 5999/5999 [00:47<00:00, 125.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run DL model: 193.61896872520447 seconds\n",
      "3014319\n"
     ]
    }
   ],
   "source": [
    "stopwatch.start()\n",
    "F.save_traces_mea_new(RECORDING_PATH, SCALED_TRACES_PATH, start_ms=TRAINING_MS[0], end_ms=TRAINING_MS[1])\n",
    "stopwatch.stop()\n",
    "\n",
    "stopwatch.start()\n",
    "model = ModelSpikeSorter.load(MODEL_PATH)\n",
    "model.compile(NUM_ELECS, MODEL_PATH)\n",
    "stopwatch.stop()\n",
    "\n",
    "stopwatch.start()\n",
    "F.run_dl_model(MODEL_PATH, SCALED_TRACES_PATH, MODEL_TRACES_PATH, MODEL_OUTPUTS_PATH)\n",
    "stopwatch.stop()\n",
    "\n",
    "stopwatch.start()\n",
    "F.NUM_ELECS = NUM_ELECS\n",
    "F.SAMP_FREQ = SAMP_FREQ\n",
    "F.FRONT_BUFFER = FRONT_BUFFER\n",
    "F.STRINGENT_THRESH = STRINGENT_THRESH\n",
    "F.STRINGENT_THRESH_LOGIT = STRINGENT_THRESH_LOGIT\n",
    "F.extract_crossings(MODEL_OUTPUTS_PATH, ALL_CROSSINGS_PATH,\n",
    "                    ELEC_CROSSINGS_IND_PATH)\n",
    "stopwatch.stop()\n",
    "print(f\"Time to run DL model: {stopwatch.duration} seconds\")\n",
    "\n",
    "# Sanity check that there are stringent detections\n",
    "print(len(np.load(ALL_CROSSINGS_PATH, allow_pickle=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwatch.start()\n",
    "\n",
    "# No user inputs here. Run after running DL model\n",
    "ALL_CLOSEST_ELECS = []\n",
    "for elec in range(NUM_ELECS):\n",
    "    elec_ind = []\n",
    "    dists = []\n",
    "    x1, y1 = ELEC_LOCS[elec]\n",
    "    for elec2 in range(RECORDING.get_num_channels()):\n",
    "        if elec == elec2:\n",
    "            continue\n",
    "        x2, y2 = ELEC_LOCS[elec2]\n",
    "        dists.append(np.sqrt((x2 - x1)**2 + (y2 - y1)**2))\n",
    "        elec_ind.append(elec2)\n",
    "    order = np.argsort(dists)\n",
    "    ALL_CLOSEST_ELECS.append(np.array(elec_ind)[order])   \n",
    "# \n",
    "TRACES = np.load(MODEL_TRACES_PATH, mmap_mode=\"r\")\n",
    "FILT_TRACES = np.load(SCALED_TRACES_PATH, mmap_mode=\"r\")  # called FILT_TRACES, but these are not actually filtered\n",
    "OUTPUTS = np.load(MODEL_OUTPUTS_PATH, mmap_mode=\"r\")\n",
    "ALL_CROSSINGS = np.load(ALL_CROSSINGS_PATH, allow_pickle=True)\n",
    "ELEC_CROSSINGS_IND = np.load(ELEC_CROSSINGS_IND_PATH, allow_pickle=True)\n",
    "\n",
    "ALL_CROSSINGS = [tuple(cross) for cross in ALL_CROSSINGS]\n",
    "ELEC_CROSSINGS_IND = [tuple(ind) for ind in ELEC_CROSSINGS_IND]  # [(elec's cross times ind in all_crossings)]\n",
    "\n",
    "stopwatch.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set global variables in .py\n",
    "reload(F)\n",
    "\n",
    "F.RECORDING = RECORDING\n",
    "F.MEA = True\n",
    "F.STRINGENT_THRESH = STRINGENT_THRESH\n",
    "F.STRINGENT_THRESH_LOGIT = STRINGENT_THRESH_LOGIT\n",
    "F.LOOSE_THRESH = LOOSE_THRESH\n",
    "F.LOOSE_THRESH_LOGIT = LOOSE_THRESH_LOGIT\n",
    "F.INFERENCE_SCALING_NUMERATOR = INFERENCE_SCALING_NUMERATOR\n",
    "\n",
    "# F.CHANS_RMS = CHANS_RMS\n",
    "F.SAMP_FREQ = SAMP_FREQ\n",
    "F.NUM_ELECS = NUM_ELECS\n",
    "F.ELEC_LOCS = ELEC_LOCS\n",
    "\n",
    "F.ALL_CLOSEST_ELECS = ALL_CLOSEST_ELECS\n",
    "\n",
    "F.FRONT_BUFFER = FRONT_BUFFER\n",
    "F.OUTPUT_WINDOW_HALF_SIZE = OUTPUT_WINDOW_HALF_SIZE\n",
    "\n",
    "F.N_BEFORE = F.N_AFTER = round(0.5 * SAMP_FREQ)  # Window for looking for electrode codetections\n",
    "F.MIN_ELECS_FOR_ARRAY_NOISE = max(100, round(0.1 * NUM_ELECS))\n",
    "F.MIN_ELECS_FOR_SEQ_NOISE = max(50, round(0.05 * NUM_ELECS))\n",
    "F.PRE_MEDIAN_FRAMES = PRE_MEDIAN_FRAMES\n",
    "\n",
    "F.MIN_ACTIVITY = 0.05 * (TRAINING_MS[1] - TRAINING_MS[0]) / 1000\n",
    "\n",
    "# If doing on new recording, these should be set after ## Full run - DL model\n",
    "F.TRACES = TRACES\n",
    "F.OUTPUTS = OUTPUTS\n",
    "F.ALL_CROSSINGS = ALL_CROSSINGS\n",
    "F.ELEC_CROSSINGS_IND = ELEC_CROSSINGS_IND\n",
    "\n",
    "# Different parameters for MEA\n",
    "F.MIN_AMP_DIST_P = -1\n",
    "F.MAX_AMP_MEDIAN_DIFF_SPIKES = F.MAX_AMP_MEDIAN_DIFF_SEQUENCES = 0.65\n",
    "F.MAX_LATENCY_DIFF_SPIKES = F.MAX_LATENCY_DIFF_SEQUENCES = 3.5\n",
    "F.CLIP_LATENCY_DIFF = 7\n",
    "F.CLIP_AMP_MEDIAN_DIFF = 1.3\n",
    "F.MAX_ROOT_AMP_MEDIAN_STD_SPIKES = 2.5\n",
    "F.MAX_ROOT_AMP_MEDIAN_STD_SEQUENCES = np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 772/772 [01:11<00:00, 10.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162 sequences before merging\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59980/59980 [02:01<00:00, 494.07it/s]\n",
      "100%|██████████| 134/134 [00:10<00:00, 12.72it/s]\n",
      "100%|██████████| 42/42 [00:00<00:00, 47.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 sequences after first merging\n",
      "\n",
      "Merged 29 with 41\n",
      "Latency diff: 0.14. Amp median diff: 0.08\n",
      "Amp dist p-value 0.3882\n",
      "#spikes:\n",
      "Merge base: 242, Add: 363, Overlaps: 1\n",
      "After merging: 604\n",
      "\n",
      "Merged 45 with 43\n",
      "Latency diff: 0.18. Amp median diff: 0.12\n",
      "Amp dist p-value 0.8174\n",
      "#spikes:\n",
      "Merge base: 751, Add: 478, Overlaps: 5\n",
      "After merging: 1226\n",
      "\n",
      "Merged 44 with [45, 43]\n",
      "Latency diff: 0.25. Amp median diff: 0.09\n",
      "Amp dist p-value 0.1826\n",
      "#spikes:\n",
      "Merge base: 781, Add: 1226, Overlaps: 12\n",
      "After merging: 1988\n",
      "\n",
      "Merged [29, 41] with 39\n",
      "Latency diff: 0.24. Amp median diff: 0.12\n",
      "Amp dist p-value 0.6890\n",
      "#spikes:\n",
      "Merge base: 604, Add: 115, Overlaps: 8\n",
      "After merging: 711\n",
      "\n",
      "Merged 31 with 35\n",
      "Latency diff: 0.36. Amp median diff: 0.15\n",
      "Amp dist p-value 2.6953\n",
      "#spikes:\n",
      "Merge base: 19, Add: 168, Overlaps: 0\n",
      "After merging: 187\n",
      "\n",
      "Merged 24 with 26\n",
      "Latency diff: 0.76. Amp median diff: 0.08\n",
      "Amp dist p-value 0.2090\n",
      "#spikes:\n",
      "Merge base: 126, Add: 65, Overlaps: 29\n",
      "After merging: 163\n",
      "\n",
      "Merged 33 with 23\n",
      "Latency diff: 0.45. Amp median diff: 0.13\n",
      "Amp dist p-value 0.9419\n",
      "#spikes:\n",
      "Merge base: 63, Add: 89, Overlaps: 0\n",
      "After merging: 149\n",
      "\n",
      "Merged 11 with 27\n",
      "Latency diff: 0.13. Amp median diff: 0.21\n",
      "Amp dist p-value 1.6270\n",
      "#spikes:\n",
      "Merge base: 203, Add: 145, Overlaps: 0\n",
      "After merging: 346\n",
      "\n",
      "Merged 2 with 10\n",
      "Latency diff: 0.42. Amp median diff: 0.15\n",
      "Amp dist p-value 0.2727\n",
      "#spikes:\n",
      "Merge base: 31, Add: 48, Overlaps: 0\n",
      "After merging: 79\n",
      "\n",
      "Merged 30 with [2, 10]\n",
      "Latency diff: 0.29. Amp median diff: 0.16\n",
      "Amp dist p-value 0.0197\n",
      "#spikes:\n",
      "Merge base: 92, Add: 79, Overlaps: 2\n",
      "After merging: 166\n",
      "\n",
      "Merged 37 with [31, 35]\n",
      "Latency diff: 0.77. Amp median diff: 0.10\n",
      "Amp dist p-value 0.1921\n",
      "#spikes:\n",
      "Merge base: 172, Add: 187, Overlaps: 4\n",
      "After merging: 345\n",
      "\n",
      "Merged [11, 27] with 17\n",
      "Latency diff: 0.26. Amp median diff: 0.19\n",
      "Amp dist p-value 1.4355\n",
      "#spikes:\n",
      "Merge base: 346, Add: 61, Overlaps: 0\n",
      "After merging: 406\n",
      "\n",
      "Merged 22 with 3\n",
      "Latency diff: 0.33. Amp median diff: 0.18\n",
      "Amp dist p-value 0.8276\n",
      "#spikes:\n",
      "Merge base: 196, Add: 74, Overlaps: 0\n",
      "After merging: 270\n",
      "\n",
      "Merged 28 with 1\n",
      "Latency diff: 0.67. Amp median diff: 0.19\n",
      "Amp dist p-value 0.9033\n",
      "#spikes:\n",
      "Merge base: 238, Add: 36, Overlaps: 0\n",
      "After merging: 274\n",
      "\n",
      "Merged [24, 26] with 16\n",
      "Latency diff: 1.35. Amp median diff: 0.13\n",
      "Amp dist p-value 0.9141\n",
      "#spikes:\n",
      "Merge base: 163, Add: 31, Overlaps: 5\n",
      "After merging: 187\n",
      "\n",
      "Merged 40 with [33, 23]\n",
      "Latency diff: 1.31. Amp median diff: 0.56\n",
      "Amp dist p-value 0.1552\n",
      "#spikes:\n",
      "Merge base: 246, Add: 149, Overlaps: 3\n",
      "After merging: 392\n",
      "\n",
      "Merged 25 with 5\n",
      "Latency diff: 2.03. Amp median diff: 0.57\n",
      "Amp dist p-value 0.0303\n",
      "#spikes:\n",
      "Merge base: 23, Add: 35, Overlaps: 0\n",
      "After merging: 58\n",
      "\n",
      "Formed 29 merged clusters:\n",
      "cluster 0: 0\n",
      "cluster 1: 4\n",
      "cluster 2: 6\n",
      "cluster 3: 7\n",
      "cluster 4: 8\n",
      "cluster 5: 9\n",
      "cluster 6: [11, 27, 17]\n",
      "cluster 7: 12\n",
      "cluster 8: 13\n",
      "cluster 9: 14\n",
      "cluster 10: 15\n",
      "cluster 11: 18\n",
      "cluster 12: 19\n",
      "cluster 13: 20\n",
      "cluster 14: 21\n",
      "cluster 15: [22, 3]\n",
      "cluster 16: [24, 26, 16]\n",
      "cluster 17: [25, 5]\n",
      "cluster 18: [28, 1]\n",
      "cluster 19: [29, 41, 39]\n",
      "cluster 20: [30, 2, 10]\n",
      "cluster 21: 32\n",
      "cluster 22: 34\n",
      "cluster 23: 36\n",
      "cluster 24: [37, 31, 35]\n",
      "cluster 25: 38\n",
      "cluster 26: [40, 33, 23]\n",
      "cluster 27: 42\n",
      "cluster 28: [44, 45, 43]\n",
      "26 sequences after second merging\n",
      "Time to detect sequences: 407.27088832855225 seconds\n"
     ]
    }
   ],
   "source": [
    "stopwatch.start()\n",
    "\n",
    "MIN_SPIKES = max(10, 0.05 * (TRAINING_MS[1] - TRAINING_MS[0]) / 1000)\n",
    "\n",
    "##\n",
    "all_clusters = F.form_all_clusters(TRACES_TRAINING_MS)\n",
    "# utils.pickle_dump(all_clusters, ROOT_PATH / \"all_clusters.pickle\")\n",
    "# all_clusters = utils.pickle_load(ROOT_PATH / \"all_clusters.pickle\")\n",
    "\n",
    "all_clusters_reassigned = F.reassign_spikes(all_clusters, TRACES_TRAINING_MS, MIN_SPIKES)\n",
    "# utils.pickle_dump(all_clusters_reassigned, ROOT_PATH / \"all_clusters_reassigned.pickle\")\n",
    "# all_clusters_reassigned = utils.pickle_load(ROOT_PATH / \"all_clusters_reassigned.pickle\")\n",
    "\n",
    "intra_merged_clusters = F.intra_merge(all_clusters_reassigned) \n",
    "trained_sequences = F.inter_merge(intra_merged_clusters, MIN_SPIKES)\n",
    "# utils.pickle_dump(trained_sequences, ROOT_PATH / \"trained_sequences.pickle\")\n",
    "# trained_sequences = utils.pickle_load(ROOT_PATH / \"trained_sequences.pickle\")\n",
    "stopwatch.stop()\n",
    "print(f\"Time to detect sequences: {stopwatch.duration} seconds\")\n",
    "\n",
    "# Save data\n",
    "utils.pickle_dump(all_clusters, ROOT_PATH / \"all_clusters.pickle\")\n",
    "utils.pickle_dump(all_clusters_reassigned, ROOT_PATH / \"all_clusters_reassigned.pickle\")\n",
    "utils.pickle_dump(trained_sequences, ROOT_PATH / \"trained_sequences.pickle\")\n",
    "\n",
    "SEQUENCES = trained_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelSpikeSorter.load(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_sequences = utils.pickle_load(ROOT_PATH / \"trained_sequences.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set global variables in .py\n",
    "reload(F)\n",
    "\n",
    "F.RECORDING = RECORDING\n",
    "F.MEA = True\n",
    "F.STRINGENT_THRESH = STRINGENT_THRESH\n",
    "F.STRINGENT_THRESH_LOGIT = STRINGENT_THRESH_LOGIT\n",
    "F.LOOSE_THRESH = LOOSE_THRESH\n",
    "F.LOOSE_THRESH_LOGIT = LOOSE_THRESH_LOGIT\n",
    "F.INFERENCE_SCALING_NUMERATOR = INFERENCE_SCALING_NUMERATOR\n",
    "\n",
    "# F.CHANS_RMS = CHANS_RMS\n",
    "F.SAMP_FREQ = SAMP_FREQ\n",
    "F.NUM_ELECS = NUM_ELECS\n",
    "F.ELEC_LOCS = ELEC_LOCS\n",
    "\n",
    "F.ALL_CLOSEST_ELECS = ALL_CLOSEST_ELECS\n",
    "\n",
    "F.FRONT_BUFFER = FRONT_BUFFER\n",
    "F.OUTPUT_WINDOW_HALF_SIZE = OUTPUT_WINDOW_HALF_SIZE\n",
    "\n",
    "# Window for looking for electrode codetections\n",
    "F.N_BEFORE = F.N_AFTER = round(0.5 * SAMP_FREQ)\n",
    "F.MIN_ELECS_FOR_ARRAY_NOISE = max(100, round(0.1 * NUM_ELECS))\n",
    "F.MIN_ELECS_FOR_SEQ_NOISE = max(50, round(0.05 * NUM_ELECS))\n",
    "F.PRE_MEDIAN_FRAMES = PRE_MEDIAN_FRAMES\n",
    "\n",
    "F.MIN_ACTIVITY = 0.05 * (TRAINING_MS[1] - TRAINING_MS[0]) / 1000\n",
    "\n",
    "# If doing on new recording, these should be set after ## Full run - DL model\n",
    "F.TRACES = TRACES\n",
    "F.OUTPUTS = OUTPUTS\n",
    "F.ALL_CROSSINGS = ALL_CROSSINGS\n",
    "F.ELEC_CROSSINGS_IND = ELEC_CROSSINGS_IND\n",
    "\n",
    "# Different parameters for MEA\n",
    "F.MIN_AMP_DIST_P = -1\n",
    "F.MAX_AMP_MEDIAN_DIFF_SPIKES = F.MAX_AMP_MEDIAN_DIFF_SEQUENCES = 0.65\n",
    "F.MAX_LATENCY_DIFF_SPIKES = F.MAX_LATENCY_DIFF_SEQUENCES = 3.5\n",
    "F.CLIP_LATENCY_DIFF = 7\n",
    "F.CLIP_AMP_MEDIAN_DIFF = 1.3\n",
    "F.MAX_ROOT_AMP_MEDIAN_STD_SPIKES = 2.5\n",
    "F.MAX_ROOT_AMP_MEDIAN_STD_SEQUENCES = np.inf\n",
    "##\n",
    "\n",
    "\n",
    "rt_sort = F.RTSort(trained_sequences, model, SCALED_TRACES_PATH)\n",
    "rt_sort.save(ROOT_PATH / \"rt_sort.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59990/59990 [01:51<00:00, 537.21it/s]\n"
     ]
    }
   ],
   "source": [
    "all_spike_trains = F.assign_spikes_torch(trained_sequences, None, return_spikes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "trunc_trains = []\n",
    "for train in all_spike_trains:\n",
    "    trunc_trains.append(train[train < 60 * 1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1096"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = 0\n",
    "for trains in trunc_trains:\n",
    "    total += len(trains)\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(ROOT_PATH / \"test_spike_trains.npy\", np.array(trunc_trains, dtype=object))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
