"use strict";(self.webpackChunkbraindance_docs=self.webpackChunkbraindance_docs||[]).push([[21],{2718:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>s,default:()=>h,frontMatter:()=>a,metadata:()=>o,toc:()=>d});var r=n(4848),i=n(8453);const a={},s="module train.py",o={id:"docs/train.py",title:"train.py",description:"---",source:"@site/docs/docs/train.py.mdx",sourceDirName:"docs",slug:"/docs/train.py",permalink:"/brainloop/docs/docs/train.py",draft:!1,unlisted:!1,editUrl:"https://github.com/braingeneers/brainloop/docs/docs/train.py.mdx",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"spikedetector.py",permalink:"/brainloop/docs/docs/spikedetector.py"},next:{title:"trainer.py",permalink:"/brainloop/docs/docs/trainer.py"}},c={},d=[{value:"<kbd>function</kbd> <code>train_detection_model</code>",id:"function-train_detection_model",level:2},{value:"<kbd>function</kbd> <code>main</code>",id:"function-main",level:2}];function l(e){const t={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",ul:"ul",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)("a",{href:"https://github.com/braingeneers/brainloop/blob/main/brainloop/core/spikedetector/train.py#L0",children:(0,r.jsx)("img",{align:"right",style:{float:"right"},src:"https://img.shields.io/badge/-source-cccccc?style=flat-square"})}),"\n",(0,r.jsx)(t.header,{children:(0,r.jsxs)(t.h1,{id:"module-trainpy",children:[(0,r.jsx)("kbd",{children:"module"})," ",(0,r.jsx)(t.code,{children:"train.py"})]})}),"\n",(0,r.jsx)(t.hr,{}),"\n",(0,r.jsx)("a",{href:"https://github.com/braingeneers/brainloop/blob/main/brainloop/core/spikedetector/train.py#L11",children:(0,r.jsx)("img",{align:"right",style:{float:"right"},src:"https://img.shields.io/badge/-source-cccccc?style=flat-square"})}),"\n",(0,r.jsxs)(t.h2,{id:"function-train_detection_model",children:[(0,r.jsx)("kbd",{children:"function"})," ",(0,r.jsx)(t.code,{children:"train_detection_model"})]}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-python",children:"train_detection_model(\n    recordings: list,\n    dl_folder_name='dl_folder',\n    validation_recording=None,\n    thresh_amp=18.88275,\n    thresh_std=0.6,\n    sample_size_ms=10,\n    recording_spike_before_ms=2,\n    recording_spike_after_ms=2,\n    samples_per_waveform=2,\n    num_wfs_probs=[0.5, 0.3, 0.12, 0.06, 0.02],\n    isi_wf_min_ms=0.2,\n    isi_wf_max_ms=None,\n    learning_rate=0.000776,\n    momentum=0.85,\n    training_thresh=0.01,\n    learning_rate_patience=5,\n    learning_rate_decay=0.4,\n    epoch_patience=10,\n    max_num_epochs=200,\n    batch_size=1,\n    num_workers=0,\n    shuffle=True,\n    training_random_seed=231,\n    input_scale=0.01,\n    input_scale_decay=0.1,\n    device='cuda',\n    dtype=torch.float16,\n    **run_kilosort2_kwargs\n)\n"})}),"\n",(0,r.jsx)(t.p,{children:"Train a DL detection model with :recording_files:"}),"\n",(0,r.jsx)(t.p,{children:"Params:  General parameters:  recordings:  A list containing the recordings to use for training the detection model. The length recordings must be at least two.   Each element can be one of the following:"}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsx)(t.li,{children:"Path to a recording file in .h5 or .nwb format"}),"\n",(0,r.jsx)(t.li,{children:"Path to a folder containing \u201csorted.npz\u201d and \u201cscaled_traces.npy\u201d.   This is used if the recordings have already been sorted, and you do not want to re-sort them."}),"\n",(0,r.jsxs)(t.li,{children:["A recording object loaded with SpikeInterface. See SpikeInterface's Extractor Module (",(0,r.jsx)(t.a,{href:"https://spikeinterface.readthedocs.io/en/latest/modules/extractors.html",children:"https://spikeinterface.readthedocs.io/en/latest/modules/extractors.html"}),") for details  dl_folder_name:  For each recording that needs to be sorted, the results will be stored in the same folder as the recording in the folder dl_folder_name"]}),"\n"]}),"\n",(0,r.jsx)(t.p,{children:"Dataset parameters:  thresh_amp:  Only waveforms whose amplitude is at least thresh_amp (microvolts) will be selected to train and test the detection model  thresh_std:  Only waveforms in which the standard deviation of the trough divided by the amplitude is at most thresh_std will be selected to train and test the detection model  sample_size_ms:  The size of the input samples fed into the detection model (milliseconds)  recording_spike_before_ms:  When extracting noise from the recording, do not extract if there was a spike within recording_spike_before_ms milliseconds  recording_spike_after_ms:  When extracting noise from the recording, do not extract if there is a spike within recording_spike_after_ms milliseconds"}),"\n",(0,r.jsxs)(t.p,{children:["Training parameters:  samples_per_waveform:  The number of samples in an epoch equals samples_per_waveform * the total number of waveformsrms  num_wfs_probs:  The ith element refers to the probability (decimal) that i waveforms will appear in a training or validation sample.  isi_wf_min_ms:  If multiple waveforms are pasted into a sample, they must be at least isi_wf_min_ms milliseconds apart  isi_wf_max_ms:  If multiple waveforms are pasted into a sample, one waveform will be at most isi_wf_max_ms milliseconds apart from another waveform  If None, there is no limit  learning_rate:  The learning rate to use for training   momentum:  The momentum value to use for training. See ",(0,r.jsx)(t.a,{href:"https://pytorch.org/docs/stable/generated/torch.optim.SGD.html",children:"https://pytorch.org/docs/stable/generated/torch.optim.SGD.html"}),"  training_thresh:  If the validation loss does not decrease by training_thresh after an epoch, the patience counter increases by 1  If it does, the patience counter resets to 0  learning_rate_decay and learning_rate_patience:  If the patience counter reaches learning_rate_patience, the learning rate decreases by a factor of learning_rate_decay  epoch_patience:  If the patience counter reacehs epoch_patience, training stops for the recording  max_num_epochs:  Training stops after max_num_epochs even if the validation loss continues to decrease  batch_size:  The batch size used for training  num_workers:  The number of workers used to load data. See ",(0,r.jsx)(t.a,{href:"https://pytorch.org/docs/stable/data.html#single-and-multi-process-data-loading",children:"https://pytorch.org/docs/stable/data.html#single-and-multi-process-data-loading"}),"  shuffle:  Whether to randomly shuffle the training and validation data for each epoch  training_random_seed  The random seed set before trainings starts for reproducibility"]}),"\n",(0,r.jsx)(t.p,{children:"Detection model parameters:  input_scale:  The recording traces (\u03bcV) are multiplied by input_scale before being inputted into the detection model.   If the training process indicates \u201cnan\u201d for the loss, then decrease input_scale because the traces are too large for the detection model.   input_scale_decay:  If loss is np.nan, reduce input_scale by factor of input_scale_decay"}),"\n",(0,r.jsx)(t.p,{children:'General parameters:  device  The device to use. "cuda" for GPU and "cpu" for CPU  dtype  The data type to use'}),"\n",(0,r.jsx)(t.p,{children:"run_kilosort2 parameters  See brainloop.core.spikesorter.kilosort2.py"}),"\n",(0,r.jsx)(t.hr,{}),"\n",(0,r.jsx)("a",{href:"https://github.com/braingeneers/brainloop/blob/main/brainloop/core/spikedetector/train.py#L223",children:(0,r.jsx)("img",{align:"right",style:{float:"right"},src:"https://img.shields.io/badge/-source-cccccc?style=flat-square"})}),"\n",(0,r.jsxs)(t.h2,{id:"function-main",children:[(0,r.jsx)("kbd",{children:"function"})," ",(0,r.jsx)(t.code,{children:"main"})]}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-python",children:"main()\n"})}),"\n",(0,r.jsx)(t.hr,{}),"\n",(0,r.jsx)(t.p,{children:(0,r.jsxs)(t.em,{children:["This file was automatically generated via ",(0,r.jsx)(t.a,{href:"https://github.com/ml-tooling/lazydocs",children:"lazydocs"}),"."]})})]})}function h(e={}){const{wrapper:t}={...(0,i.R)(),...e.components};return t?(0,r.jsx)(t,{...e,children:(0,r.jsx)(l,{...e})}):l(e)}},8453:(e,t,n)=>{n.d(t,{R:()=>s,x:()=>o});var r=n(6540);const i={},a=r.createContext(i);function s(e){const t=r.useContext(a);return r.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),r.createElement(a.Provider,{value:t},e.children)}}}]);